{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aShYousef/Freecode2/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0vxsAUbTOBc",
        "outputId": "c6f81bc2-3abf-4f39-cb58-b285a63efecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive to /content/drive...\n",
            "Failed to mount Drive: mount failed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "MOUNT_PATH = '/content/drive'\n",
        "\n",
        "try:\n",
        "    print(f\"Mounting Google Drive to {MOUNT_PATH}...\")\n",
        "    drive.mount(MOUNT_PATH, force_remount=True)\n",
        "    print(\"Drive mounted successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to mount Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZI7H50IahuS",
        "outputId": "6941cb25-ba2b-4daa-b46e-59e2b19ee0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Ngrok Authenticated.\n",
            "Restarting Server...\n",
            "App Updated! Open here: https://miss-centered-rustlingly.ngrok-free.dev\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "=============================================================================\n",
        "Project Name: Cloud-Based Distributed Data Processing Service\n",
        "Authors: Alaa Yousef & Misk Ashour\n",
        "Supervised By: Dr. Rebhi S. Baraka\n",
        "Description: A Streamlit app using Apache Spark to analyze data and simulate\n",
        "             distributed computing scalability.\n",
        "=============================================================================\n",
        "\"\"\"\n",
        "\n",
        "# 1. Install necessary libraries\n",
        "!pip install pyspark streamlit pyngrok\n",
        "\n",
        "# 2. Import libraries for the notebook environment\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pyngrok import ngrok\n",
        "from google.colab import drive, userdata\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# 3. Authenticate Ngrok (to make the app public)\n",
        "try:\n",
        "    ngrok_auth_token = userdata.get('NGROK_TOKEN')\n",
        "    if ngrok_auth_token:\n",
        "        ngrok.set_auth_token(ngrok_auth_token)\n",
        "        print(\"Ngrok Authenticated.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# =============================================================================\n",
        "# STREAMLIT APPLICATION CODE\n",
        "# This string contains the entire web app logic that will be saved to app.py\n",
        "# =============================================================================\n",
        "streamlit_application_code = r'''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# --- Page Setup ---\n",
        "st.set_page_config(page_title=\"Distributed Data Service\", layout=\"wide\")\n",
        "\n",
        "# --- Constants ---\n",
        "RESULTS_STORAGE_PATH = \"/content/drive/MyDrive/University_Project_Results\"\n",
        "DEMO_DATASET_PATH = \"/content/large_dataset_150MB.csv\"\n",
        "\n",
        "# --- Helper Function: Start Spark ---\n",
        "def initialize_spark_session(app_name=\"MasterNode\"):\n",
        "    # Create a Spark session with 4GB memory\n",
        "    return SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(app_name) \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Initialize the main Spark session\n",
        "spark_session = initialize_spark_session()\n",
        "\n",
        "# --- Sidebar: System Control & Info ---\n",
        "with st.sidebar:\n",
        "    st.header(\"System Control\")\n",
        "    st.info(\"System Status: Active\")\n",
        "\n",
        "    st.markdown(\"### Developed By:\\n**Alaa Yousef & Misk Ashour**\")\n",
        "    st.write(\"---\")\n",
        "\n",
        "    # Checkbox to switch to \"Report Mode\" (clean view)\n",
        "    is_report_mode = st.checkbox(\"Generate Report Mode\", value=False)\n",
        "\n",
        "    # Button: Save all results to Google Drive\n",
        "    if st.button(\"Save Results to Drive\"):\n",
        "        # Create folder if it doesn't exist\n",
        "        if not os.path.exists(RESULTS_STORAGE_PATH):\n",
        "            try:\n",
        "                os.makedirs(RESULTS_STORAGE_PATH, exist_ok=True)\n",
        "            except:\n",
        "                st.error(\"Could not create Drive folder. Make sure Drive is mounted.\")\n",
        "\n",
        "        if os.path.exists(RESULTS_STORAGE_PATH):\n",
        "            # Create a timestamped folder for this run\n",
        "            timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "            output_directory = os.path.join(RESULTS_STORAGE_PATH, f\"Report_{timestamp_str}\")\n",
        "            os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "            saved_files_list = []\n",
        "\n",
        "            # Save the Original Dataset\n",
        "            if \"dataset_dataframe\" in st.session_state and st.session_state.dataset_dataframe is not None:\n",
        "                st.session_state.dataset_dataframe.to_csv(os.path.join(output_directory, \"Original_Dataset.csv\"), index=False)\n",
        "                saved_files_list.append(\"Original Dataset\")\n",
        "\n",
        "            # Save Statistics\n",
        "            if \"dataset_dataframe\" in st.session_state and st.session_state.dataset_dataframe is not None:\n",
        "                statistics_dataframe = st.session_state.dataset_dataframe.describe()\n",
        "                statistics_dataframe.to_csv(os.path.join(output_directory, \"Statistics.csv\"))\n",
        "                saved_files_list.append(\"Statistics\")\n",
        "\n",
        "            # Save ML Results\n",
        "            if \"model_performance_results\" in st.session_state and st.session_state.model_performance_results is not None:\n",
        "                st.session_state.model_performance_results.to_csv(os.path.join(output_directory, \"ML_Results.csv\"), index=False)\n",
        "                saved_files_list.append(\"ML Results\")\n",
        "\n",
        "            # Save Scalability Results\n",
        "            if \"scalability_results\" in st.session_state and st.session_state.scalability_results is not None:\n",
        "                st.session_state.scalability_results.to_csv(os.path.join(output_directory, \"Scalability_Results.csv\"), index=False)\n",
        "                saved_files_list.append(\"Scalability Data\")\n",
        "\n",
        "            # Feedback to user\n",
        "            if saved_files_list:\n",
        "                st.success(f\"Saved successfully to Drive folder: Report_{timestamp_str}\")\n",
        "                st.write(f\"Saved Files: {', '.join(saved_files_list)}\")\n",
        "            else:\n",
        "                st.warning(\"No data loaded to save yet!\")\n",
        "        else:\n",
        "            st.error(\"Drive path not accessible.\")\n",
        "\n",
        "# --- Main Page Title ---\n",
        "if is_report_mode:\n",
        "    st.title(\"Final Project Report\")\n",
        "else:\n",
        "    st.title(\"Cloud-Based Distributed Data Processing Service\")\n",
        "\n",
        "# Initialize session state for data\n",
        "if \"dataset_dataframe\" not in st.session_state:\n",
        "    st.session_state.dataset_dataframe = None\n",
        "\n",
        "# --- Section 1: Data Ingestion (Upload or Demo) ---\n",
        "if not is_report_mode:\n",
        "    st.subheader(\"1. Data Ingestion\")\n",
        "    ingestion_source = st.radio(\"Input Source:\", [\"Upload File\", \"Use Demo Data (Fast)\"], horizontal=True)\n",
        "\n",
        "    loaded_dataframe = None\n",
        "    if ingestion_source == \"Upload File\":\n",
        "        # Handle file upload\n",
        "        uploaded_file = st.file_uploader(\"Upload CSV/JSON\", type=['csv','json'])\n",
        "        if uploaded_file:\n",
        "            if uploaded_file.name.endswith('.csv'):\n",
        "                loaded_dataframe = pd.read_csv(uploaded_file)\n",
        "            else:\n",
        "                loaded_dataframe = pd.read_json(uploaded_file)\n",
        "    else:\n",
        "        # Load Demo Data from server\n",
        "        if st.button(\"Load Demo File\"):\n",
        "            if os.path.exists(DEMO_DATASET_PATH):\n",
        "                loaded_dataframe = pd.read_csv(DEMO_DATASET_PATH)\n",
        "                st.success(\"Demo data loaded from local server.\")\n",
        "            else:\n",
        "                st.error(\"Demo file missing. Please run generation script.\")\n",
        "\n",
        "    if loaded_dataframe is not None:\n",
        "        st.session_state.dataset_dataframe = loaded_dataframe\n",
        "        st.success(f\"Ingested {len(loaded_dataframe)} records.\")\n",
        "\n",
        "# --- Main Logic: If data is loaded, show tabs ---\n",
        "if st.session_state.dataset_dataframe is not None:\n",
        "    active_dataframe = st.session_state.dataset_dataframe\n",
        "\n",
        "    # Function to show stats\n",
        "    def display_statistics():\n",
        "        st.header(\"Statistical Analysis\")\n",
        "        # Use Pandas describe for quick stats\n",
        "        st.write(active_dataframe.describe())\n",
        "\n",
        "    # Function for Machine Learning\n",
        "    def display_machine_learning_module():\n",
        "        st.header(\"Machine Learning Models\")\n",
        "        # Get numeric columns only\n",
        "        numeric_columns = [col for col in active_dataframe.columns if pd.api.types.is_numeric_dtype(active_dataframe[col])]\n",
        "\n",
        "        if len(numeric_columns) > 1:\n",
        "            col1, col2 = st.columns([1, 2])\n",
        "            target_column = col1.selectbox(\"Target (Y)\", numeric_columns)\n",
        "            feature_columns = col2.multiselect(\"Features (X)\", [col for col in numeric_columns if col != target_column])\n",
        "\n",
        "            st.session_state.feature_columns = feature_columns\n",
        "            st.session_state.target_column = target_column\n",
        "\n",
        "            if st.button(\"Train Models\"):\n",
        "                if feature_columns:\n",
        "                    # Convert Pandas DF to Spark DF for processing\n",
        "                    temp_spark_session = initialize_spark_session()\n",
        "                    spark_dataframe = temp_spark_session.createDataFrame(active_dataframe)\n",
        "\n",
        "                    # Create feature vector\n",
        "                    vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "                    processed_data = vector_assembler.transform(spark_dataframe).select(\"features\", target_column).withColumnRenamed(target_column, \"label\")\n",
        "\n",
        "                    # Define the models to train\n",
        "                    ml_models = {\n",
        "                        \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                        \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                        \"Random Forest\": RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                        \"K-Means\": KMeans(featuresCol=\"features\", k=3)\n",
        "                    }\n",
        "\n",
        "                    performance_metrics = []\n",
        "                    progress_bar = st.progress(0)\n",
        "\n",
        "                    # Loop through models and train them\n",
        "                    for index, (model_name, model_instance) in enumerate(ml_models.items()):\n",
        "                        time.sleep(0.5) # FIX: Add sleep to prevent flickering\n",
        "                        start_time = time.time()\n",
        "                        try:\n",
        "                            model_instance.fit(processed_data)\n",
        "                            duration_seconds = time.time() - start_time\n",
        "                            execution_status = \"Success\"\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error training {model_name}: {e}\") # FIX: Log error instead of silent fail\n",
        "                            duration_seconds = 0\n",
        "                            execution_status = \"Failed\"\n",
        "\n",
        "                        performance_metrics.append({\n",
        "                            \"Model\": model_name,\n",
        "                            \"Time (s)\": round(duration_seconds, 4),\n",
        "                            \"Status\": execution_status\n",
        "                        })\n",
        "                        progress_bar.progress((index + 1) / 4)\n",
        "\n",
        "                    st.session_state.model_performance_results = pd.DataFrame(performance_metrics)\n",
        "\n",
        "            if \"model_performance_results\" in st.session_state:\n",
        "                st.table(st.session_state.model_performance_results)\n",
        "\n",
        "    # Function for Scalability Test\n",
        "    def display_scalability_simulation():\n",
        "        st.header(\"Scalability Simulation\")\n",
        "        feature_columns = st.session_state.get('feature_columns', [])\n",
        "        target_column = st.session_state.get('target_column', None)\n",
        "\n",
        "        if st.button(\"Run Cluster Test\"):\n",
        "            if not feature_columns:\n",
        "                st.error(\"Configure ML tab first.\")\n",
        "            else:\n",
        "                node_counts = [1, 2, 4, 8]\n",
        "                scalability_metrics = []\n",
        "\n",
        "                # Stop current session to free resources\n",
        "                global spark_session\n",
        "                spark_session.stop()\n",
        "\n",
        "                progress_bar = st.progress(0)\n",
        "                # Loop through different node counts (Simulating Cluster Size)\n",
        "                for index, node_count in enumerate(node_counts):\n",
        "                    gc.collect()\n",
        "                    time.sleep(0.5) # FIX: Add sleep to prevent flickering\n",
        "                    try:\n",
        "                        # Create a new isolated Spark session with N threads (nodes)\n",
        "                        isolated_spark_session = SparkSession.builder.master(f\"local[{node_count}]\").appName(f\"Node_{node_count}\").getOrCreate()\n",
        "                        node_dataframe = isolated_spark_session.createDataFrame(active_dataframe)\n",
        "\n",
        "                        # Prepare data\n",
        "                        vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "                        training_data = vector_assembler.transform(node_dataframe).select(\"features\", target_column).withColumnRenamed(target_column, \"label\")\n",
        "\n",
        "                        # Measure training time\n",
        "                        start_time = time.time()\n",
        "                        LinearRegression(featuresCol=\"features\", labelCol=\"label\").fit(training_data)\n",
        "                        end_time = time.time()\n",
        "\n",
        "                        latency = end_time - start_time\n",
        "\n",
        "                        # Calculate Speedup and Efficiency\n",
        "                        baseline_latency = scalability_metrics[0][\"Time\"] if index > 0 else latency\n",
        "                        speedup_factor = baseline_latency / latency if latency > 0 else 0\n",
        "\n",
        "                        scalability_metrics.append({\n",
        "                            \"Nodes\": node_count,\n",
        "                            \"Time\": latency,\n",
        "                            \"Speedup\": speedup_factor,\n",
        "                            \"Efficiency\": speedup_factor / node_count\n",
        "                        })\n",
        "                        isolated_spark_session.stop()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error on node {node_count}: {e}\") # FIX: Log error\n",
        "                        pass\n",
        "                    progress_bar.progress((index + 1) / 4)\n",
        "\n",
        "                # Restart main session\n",
        "                spark_session = initialize_spark_session()\n",
        "                st.session_state.scalability_results = pd.DataFrame(scalability_metrics)\n",
        "\n",
        "        if \"scalability_results\" in st.session_state:\n",
        "            results_df = st.session_state.scalability_results\n",
        "            st.dataframe(results_df)\n",
        "            # Plot charts\n",
        "            chart_col1, chart_col2 = st.columns(2)\n",
        "            chart_col1.line_chart(results_df.set_index(\"Nodes\")[\"Speedup\"])\n",
        "            chart_col2.line_chart(results_df.set_index(\"Nodes\")[\"Efficiency\"])\n",
        "            st.bar_chart(results_df.set_index(\"Nodes\")[\"Time\"])\n",
        "\n",
        "    # --- Render Tabs ---\n",
        "    if is_report_mode:\n",
        "        display_statistics()\n",
        "        st.markdown(\"---\")\n",
        "        display_machine_learning_module()\n",
        "        st.markdown(\"---\")\n",
        "        display_scalability_simulation()\n",
        "    else:\n",
        "        tab_stats, tab_ml, tab_scale = st.tabs([\"Statistics\", \"ML Models\", \"Scalability\"])\n",
        "        with tab_stats: display_statistics()\n",
        "        with tab_ml: display_machine_learning_module()\n",
        "        with tab_scale: display_scalability_simulation()\n",
        "'''\n",
        "\n",
        "# =============================================================================\n",
        "# SAVE AND RUN\n",
        "# =============================================================================\n",
        "\n",
        "# 4. Save the string above to a file named 'app.py'\n",
        "with open(\"app.py\", \"w\") as file_handle:\n",
        "    file_handle.write(streamlit_application_code)\n",
        "\n",
        "print(\"Restarting Server...\")\n",
        "\n",
        "# 5. Kill any existing Streamlit process to avoid conflicts\n",
        "get_ipython().system_raw('pkill -9 streamlit')\n",
        "time.sleep(2)\n",
        "\n",
        "# 6. Run Streamlit in the background\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 --server.maxUploadSize 2000 &')\n",
        "\n",
        "# 7. Create Ngrok Tunnel to expose the app\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    public_url = ngrok.connect(8501).public_url\n",
        "    print(f\"App Updated! Open here: {public_url}\")\n",
        "except Exception as error_message:\n",
        "    print(f\"Error: {error_message}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "OUTPUT_FILENAME = \"large_dataset_150MB.csv\"\n",
        "\n",
        "ROW_COUNT = 600_000\n",
        "COLUMN_COUNT = 6\n",
        "\n",
        "print(f\"Generating dataset with {ROW_COUNT} rows...\")\n",
        "\n",
        "synthetic_data = np.random.randn(ROW_COUNT, COLUMN_COUNT)\n",
        "\n",
        "column_names = [f'Feature_{i}' for i in range(1, COLUMN_COUNT)] + ['Target']\n",
        "\n",
        "dataset_dataframe = pd.DataFrame(\n",
        "    synthetic_data,\n",
        "    columns=column_names\n",
        ")\n",
        "\n",
        "dataset_dataframe.to_csv(OUTPUT_FILENAME, index=False)\n",
        "\n",
        "file_size_bytes = os.path.getsize(OUTPUT_FILENAME)\n",
        "file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"Success! New file generated. Size: {file_size_mb:.2f} MB\")\n",
        "print(\"Action Required: Return to the application, click 'Load Demo File', and run the simulation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tka5t1NgqW7",
        "outputId": "d7dd20b4-6eae-43c3-98fb-a8a00de6f34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset with 600000 rows...\n",
            "Success! New file generated. Size: 67.40 MB\n",
            "Action Required: Return to the application, click 'Load Demo File', and run the simulation.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
