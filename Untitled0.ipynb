{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVbsXd//8HCGPqeMCVPVNJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aShYousef/Freecode2/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "MOUNT_PATH = '/content/drive'\n",
        "\n",
        "try:\n",
        "    print(f\"Mounting Google Drive to {MOUNT_PATH}...\")\n",
        "    drive.mount(MOUNT_PATH, force_remount=True)\n",
        "    print(\"Drive mounted successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to mount Drive: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0vxsAUbTOBc",
        "outputId": "912d6e3f-d3eb-4fff-d247-5831625f0fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive to /content/drive...\n",
            "Mounted at /content/drive\n",
            "Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark streamlit pyngrok\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pyngrok import ngrok\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "MOUNT_POINT = '/content/drive'\n",
        "\n",
        "try:\n",
        "    if os.path.exists(MOUNT_POINT):\n",
        "        drive.mount(MOUNT_POINT, force_remount=True)\n",
        "    else:\n",
        "        drive.mount(MOUNT_POINT)\n",
        "    print(f\"Drive mounted successfully at {MOUNT_POINT}\")\n",
        "except Exception as e:\n",
        "    print(f\"Drive mount failed: {e}\")\n",
        "\n",
        "try:\n",
        "    AUTH_TOKEN = userdata.get('NGROK_TOKEN')\n",
        "    if AUTH_TOKEN:\n",
        "        ngrok.set_auth_token(AUTH_TOKEN)\n",
        "        print(\"Ngrok authenticated.\")\n",
        "    else:\n",
        "        print(\"Notice: NGROK_TOKEN not found in secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ngrok authentication error: {e}\")\n",
        "\n",
        "streamlit_app_code = r'''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "st.set_page_config(page_title=\"Distributed Data System\", layout=\"wide\")\n",
        "\n",
        "BASE_OUTPUT_PATH = \"/content/drive/MyDrive/University_Project_Results\"\n",
        "\n",
        "def save_dataframe_to_drive(df, output_dir, filename):\n",
        "    try:\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        full_path = os.path.join(output_dir, filename)\n",
        "        df.to_csv(full_path, index=False)\n",
        "        return True, full_path\n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "def initialize_spark_session(app_name=\"CloudCluster\"):\n",
        "    return SparkSession.builder.master(\"local[*]\").appName(app_name).getOrCreate()\n",
        "\n",
        "spark = initialize_spark_session()\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"System Information\")\n",
        "    st.info(\"Module: Cloud & Distributed Systems\")\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "    <div style=\"background-color: #e2e3e5; padding: 10px; border-radius: 5px; border: 1px solid #d6d8db;\">\n",
        "        <h4 style=\"color: #383d41; margin:0;\">Developed by:</h4>\n",
        "        <p style=\"color: #383d41; font-weight: bold; margin:0;\">Alaa Yousef & Misk Ashour</p>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    st.write(\"---\")\n",
        "    st.success(\"Storage Status: Online\")\n",
        "\n",
        "    report_view_enabled = st.checkbox(\"Enable Report View\", value=False)\n",
        "\n",
        "    st.write(\"---\")\n",
        "\n",
        "    if st.button(\"Save Full Report to Cloud\"):\n",
        "        if \"current_dataframe\" in st.session_state and st.session_state.current_dataframe is not None:\n",
        "            current_ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "            report_dir = os.path.join(BASE_OUTPUT_PATH, f\"Report_{current_ts}\")\n",
        "\n",
        "            os.makedirs(report_dir, exist_ok=True)\n",
        "            saved_items = []\n",
        "\n",
        "            if \"current_dataframe\" in st.session_state:\n",
        "                 try:\n",
        "                    sdf = spark.createDataFrame(st.session_state.current_dataframe)\n",
        "                    stats_df = sdf.describe().toPandas()\n",
        "                    save_dataframe_to_drive(stats_df, report_dir, \"1_statistics.csv\")\n",
        "                    saved_items.append(\"Statistics\")\n",
        "                 except: pass\n",
        "\n",
        "            if \"ml_results\" in st.session_state:\n",
        "                save_dataframe_to_drive(st.session_state.ml_results, report_dir, \"2_ml_results.csv\")\n",
        "                saved_items.append(\"ML Results\")\n",
        "\n",
        "            if \"scalability_data\" in st.session_state:\n",
        "                save_dataframe_to_drive(st.session_state.scalability_data, report_dir, \"3_scalability_test.csv\")\n",
        "                saved_items.append(\"Scalability Metrics\")\n",
        "\n",
        "            st.success(f\"Report Directory Created: Report_{current_ts}\")\n",
        "            st.info(f\"Archived: {', '.join(saved_items)}\")\n",
        "        else:\n",
        "            st.error(\"No data available to persist.\")\n",
        "\n",
        "if report_view_enabled:\n",
        "    st.markdown(\"<h1 style='text-align: center;'>Project Final Report</h1>\", unsafe_allow_html=True)\n",
        "    st.markdown(f\"<h3 style='text-align: center;'>Developers: Alaa Yousef & Misk Ashour</h3>\", unsafe_allow_html=True)\n",
        "else:\n",
        "    st.title(\"Cloud-Based Distributed Data Processing Service\")\n",
        "\n",
        "if \"current_dataframe\" not in st.session_state:\n",
        "    st.session_state.current_dataframe = None\n",
        "\n",
        "if not report_view_enabled:\n",
        "    st.subheader(\"1. Dataset Ingestion\")\n",
        "    uploaded_file = st.file_uploader(\"Select Data Source\", type=[\"csv\", \"json\", \"txt\"])\n",
        "\n",
        "    if uploaded_file:\n",
        "        try:\n",
        "            filename = uploaded_file.name\n",
        "            if filename.endswith('.csv'):\n",
        "                raw_df = pd.read_csv(uploaded_file)\n",
        "            elif filename.endswith('.json'):\n",
        "                json_data = json.load(uploaded_file)\n",
        "                raw_df = pd.DataFrame(json_data if isinstance(json_data, list) else [json_data])\n",
        "            else:\n",
        "                raw_df = pd.read_csv(uploaded_file, sep=\"\\t\")\n",
        "\n",
        "            st.session_state.current_dataframe = raw_df.dropna()\n",
        "\n",
        "            save_dataframe_to_drive(st.session_state.current_dataframe, BASE_OUTPUT_PATH, f\"backup_{filename}\")\n",
        "\n",
        "            st.success(f\"Ingestion Successful. Records: {len(raw_df)}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Ingestion Failure: {e}\")\n",
        "\n",
        "if st.session_state.current_dataframe is not None:\n",
        "    pd_df = st.session_state.current_dataframe\n",
        "\n",
        "    try:\n",
        "        spark_df = spark.createDataFrame(pd_df)\n",
        "    except:\n",
        "        spark = initialize_spark_session()\n",
        "        spark_df = spark.createDataFrame(pd_df)\n",
        "\n",
        "    def view_statistics():\n",
        "        st.header(\"Statistical Analysis\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        col1.metric(\"Total Observations\", spark_df.count())\n",
        "        col2.metric(\"Feature Count\", len(spark_df.columns))\n",
        "\n",
        "        desc = spark_df.describe().toPandas()\n",
        "        st.dataframe(desc, use_container_width=True)\n",
        "\n",
        "    def view_machine_learning():\n",
        "        st.header(\"Machine Learning Execution\")\n",
        "        numeric_fields = [f.name for f in spark_df.schema.fields if f.dataType.simpleString() in ['int', 'double', 'float', 'bigint', 'long']]\n",
        "\n",
        "        c1, c2 = st.columns([1, 2])\n",
        "        target_col = c1.selectbox(\"Target Variable (Y)\", numeric_fields, key=\"target_sel\")\n",
        "        feature_cols = c2.multiselect(\"Predictors (X)\", [n for n in numeric_fields if n != target_col], key=\"feat_sel\")\n",
        "\n",
        "        st.session_state.target = target_col\n",
        "        st.session_state.features = feature_cols\n",
        "\n",
        "        if st.button(\"Execute Models\"):\n",
        "            if not feature_cols:\n",
        "                st.error(\"Please select at least one feature.\")\n",
        "            else:\n",
        "                assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "                final_data = assembler.transform(spark_df).select(\"features\", target_col).withColumnRenamed(target_col, \"label\")\n",
        "\n",
        "                models = {\n",
        "                    \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                    \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                    \"K-Means Clustering\": KMeans(featuresCol=\"features\", k=3)\n",
        "                }\n",
        "\n",
        "                logs = []\n",
        "                p_bar = st.progress(0)\n",
        "\n",
        "                for i, (name, model) in enumerate(models.items()):\n",
        "                    start_t = time.time()\n",
        "                    try:\n",
        "                        model.fit(final_data)\n",
        "                        exec_time = time.time() - start_t\n",
        "                        logs.append({\"Algorithm\": name, \"Execution Time (s)\": round(exec_time, 4), \"Status\": \"Success\"})\n",
        "                    except:\n",
        "                        logs.append({\"Algorithm\": name, \"Execution Time (s)\": 0, \"Status\": \"Failed\"})\n",
        "\n",
        "                    p_bar.progress((i + 1) / len(models))\n",
        "\n",
        "                st.session_state.ml_results = pd.DataFrame(logs)\n",
        "\n",
        "        if \"ml_results\" in st.session_state:\n",
        "            st.table(st.session_state.ml_results)\n",
        "\n",
        "    def view_scalability():\n",
        "        st.header(\"Scalability & Performance Testing\")\n",
        "\n",
        "        feats = st.session_state.get('features', [])\n",
        "        target = st.session_state.get('target', None)\n",
        "\n",
        "        if st.button(\"Run Cluster Simulation (1, 2, 4, 8 Nodes)\"):\n",
        "            if not feats:\n",
        "                st.error(\"Configuration missing in ML tab.\")\n",
        "            else:\n",
        "                st.info(\"Starting simulation...\")\n",
        "                results = []\n",
        "                nodes = [1, 2, 4, 8]\n",
        "                baseline = 0\n",
        "\n",
        "                global spark\n",
        "                spark.stop()\n",
        "\n",
        "                s_bar = st.progress(0)\n",
        "\n",
        "                for i, n in enumerate(nodes):\n",
        "                    tmp_spark = SparkSession.builder.master(f\"local[{n}]\").appName(f\"Sim_{n}\").getOrCreate()\n",
        "                    tmp_df = tmp_spark.createDataFrame(pd_df)\n",
        "\n",
        "                    vec = VectorAssembler(inputCols=feats, outputCol=\"features\")\n",
        "                    train = vec.transform(tmp_df).select(\"features\", target).withColumnRenamed(target, \"label\")\n",
        "\n",
        "                    t0 = time.time()\n",
        "                    LinearRegression(featuresCol=\"features\", labelCol=\"label\").fit(train)\n",
        "                    t_diff = time.time() - t0\n",
        "\n",
        "                    if n == 1: baseline = t_diff\n",
        "\n",
        "                    speedup = baseline / t_diff if t_diff > 0 else 0\n",
        "                    eff = speedup / n if n > 0 else 0\n",
        "\n",
        "                    results.append({\n",
        "                        \"Cluster Nodes\": n,\n",
        "                        \"Processing Time (s)\": t_diff,\n",
        "                        \"Speedup Factor\": speedup,\n",
        "                        \"Efficiency\": eff\n",
        "                    })\n",
        "\n",
        "                    tmp_spark.stop()\n",
        "                    s_bar.progress((i + 1) / 4)\n",
        "\n",
        "                spark = initialize_spark_session()\n",
        "                st.session_state.scalability_data = pd.DataFrame(results)\n",
        "                st.success(\"Simulation Complete.\")\n",
        "\n",
        "        if 'scalability_data' in st.session_state:\n",
        "            res_df = st.session_state.scalability_data\n",
        "            st.dataframe(res_df)\n",
        "\n",
        "            g1, g2 = st.columns(2)\n",
        "            with g1:\n",
        "                st.subheader(\"Speedup Analysis\")\n",
        "                st.line_chart(res_df.set_index(\"Cluster Nodes\")[\"Speedup Factor\"])\n",
        "            with g2:\n",
        "                st.subheader(\"Efficiency Analysis\")\n",
        "                st.line_chart(res_df.set_index(\"Cluster Nodes\")[\"Efficiency\"])\n",
        "\n",
        "            st.subheader(\"Execution Latency\")\n",
        "            st.bar_chart(res_df.set_index(\"Cluster Nodes\")[\"Processing Time (s)\"])\n",
        "\n",
        "    if report_view_enabled:\n",
        "        view_statistics()\n",
        "        st.markdown(\"---\")\n",
        "        view_machine_learning()\n",
        "        st.markdown(\"---\")\n",
        "        view_scalability()\n",
        "    else:\n",
        "        t1, t2, t3 = st.tabs([\"Data Statistics\", \"ML Models\", \"Scalability Test\"])\n",
        "        with t1: view_statistics()\n",
        "        with t2: view_machine_learning()\n",
        "        with t3: view_scalability()\n",
        "else:\n",
        "    st.info(\"Awaiting dataset upload to initialize pipeline.\")\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "get_ipython().system_raw('pkill -9 streamlit')\n",
        "time.sleep(2)\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 &')\n",
        "\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    url = ngrok.connect(8501).public_url\n",
        "    print(f\"Service Available at: {url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Tunneling Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY05_w9HUKq0",
        "outputId": "c4f38642-aedd-4dfb-cd9b-db3c3d7cee94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Drive mounted successfully at /content/drive\n",
            "Ngrok authenticated.\n",
            "Service Available at: https://miss-centered-rustlingly.ngrok-free.dev\n"
          ]
        }
      ]
    }
  ]
}