{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtoOEKR07g+gXpwVAPg1XS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aShYousef/Freecode2/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (Install Libraries)\n",
        "!pip install -q pyspark streamlit pyngrok\n",
        "\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”´ Ù‡Ø§Ù…: Ø¶Ø¹ Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§ Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…ØªÙŠ Ø§Ù„ØªÙ†ØµÙŠØµ\n",
        "NGROK_TOKEN = \"370cvQc2OEKtGFkRfNP3JIaHq3h_37qQ33Xwm3aPugtnLFuSA\"\n",
        "# ==========================================\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# 2. ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø·ÙˆØ± (Advanced App)\n",
        "app_content = '''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "st.set_page_config(page_title=\"Pro Cloud Data Service\", layout=\"wide\")\n",
        "\n",
        "# --- Sidebar: Configuration ---\n",
        "st.sidebar.header(\"âš™ï¸ Configuration\")\n",
        "nodes = st.sidebar.selectbox(\"Select Worker Nodes (Simulated):\", [1, 2, 4, 8])\n",
        "\n",
        "# Start Spark Session\n",
        "if \"spark\" not in st.session_state:\n",
        "    st.session_state.spark = SparkSession.builder.master(f\"local[{nodes}]\").appName(\"ProProject\").getOrCreate()\n",
        "spark = st.session_state.spark\n",
        "\n",
        "st.title(\"â˜ï¸ Cloud-Based Distributed Data Processing\")\n",
        "st.markdown(\"Upload your dataset (CSV) and run distributed Machine Learning jobs.\")\n",
        "\n",
        "# --- 1. Data Input (Upload File) ---\n",
        "st.header(\"1. Upload Dataset\")\n",
        "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Pandas Ø«Ù… ØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù€ Spark (Ù„Ù„ØªØ¨Ø³ÙŠØ· ÙÙŠ Colab)\n",
        "    pdf = pd.read_csv(uploaded_file)\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø³Ø±ÙŠØ¹: Ø­Ø°Ù Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ©\n",
        "    pdf = pdf.dropna()\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ Spark DataFrame\n",
        "    df = spark.createDataFrame(pdf)\n",
        "    st.session_state[\"df\"] = df\n",
        "\n",
        "    st.success(\"File Uploaded Successfully!\")\n",
        "    st.write(\"Preview of your data:\")\n",
        "    st.dataframe(pdf.head())\n",
        "\n",
        "    # --- 2. Descriptive Statistics ---\n",
        "    st.write(\"---\")\n",
        "    st.header(\"2. Descriptive Statistics\")\n",
        "    if st.button(\"Show Statistics\"):\n",
        "        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ¹Ø±Ø¶Ù‡Ø§\n",
        "        stats = df.describe().toPandas()\n",
        "        st.dataframe(stats)\n",
        "\n",
        "    # --- 3. Machine Learning Jobs (4 Algorithms) ---\n",
        "    st.write(\"---\")\n",
        "    st.header(\"3. Machine Learning Jobs\")\n",
        "\n",
        "    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (Ù„Ø¬Ø¹Ù„ Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø£ÙŠ Ù…Ù„Ù)\n",
        "    # Ù†Ø®ØªØ§Ø± ÙÙ‚Ø· Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
        "    numeric_cols = [c for c, t in df.dtypes if t in ('int', 'double', 'float', 'bigint')]\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        target_col = st.selectbox(\"Select Target Column (Label):\", numeric_cols)\n",
        "    with col2:\n",
        "        # Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
        "        feature_options = [c for c in numeric_cols if c != target_col]\n",
        "        feature_cols = st.multiselect(\"Select Feature Columns:\", feature_options, default=feature_options[:2])\n",
        "\n",
        "    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª (4 Required Algorithms)\n",
        "    algo_type = st.selectbox(\"Choose Machine Learning Algorithm:\",\n",
        "                             [\"1. Linear Regression\",\n",
        "                              \"2. Decision Tree Regressor\",\n",
        "                              \"3. Random Forest Regressor\",\n",
        "                              \"4. K-Means Clustering\"])\n",
        "\n",
        "    if st.button(\"Run Distributed Job\"):\n",
        "        if not feature_cols:\n",
        "            st.error(\"Please select at least one feature column.\")\n",
        "        else:\n",
        "            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Vector Assembly)\n",
        "            assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "            vec_df = assembler.transform(df)\n",
        "\n",
        "            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„\n",
        "            if \"Linear Regression\" in algo_type:\n",
        "                model = LinearRegression(featuresCol=\"features\", labelCol=target_col)\n",
        "            elif \"Decision Tree\" in algo_type:\n",
        "                model = DecisionTreeRegressor(featuresCol=\"features\", labelCol=target_col)\n",
        "            elif \"Random Forest\" in algo_type:\n",
        "                model = RandomForestRegressor(featuresCol=\"features\", labelCol=target_col)\n",
        "            elif \"K-Means\" in algo_type:\n",
        "                model = KMeans(featuresCol=\"features\", k=3)\n",
        "\n",
        "            # Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª\n",
        "            st.write(f\"ğŸš€ Running **{algo_type}** on **{nodes}** node(s)...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Fit)\n",
        "            trained_model = model.fit(vec_df)\n",
        "\n",
        "            end_time = time.time()\n",
        "            duration = end_time - start_time\n",
        "\n",
        "            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "            st.success(f\"âœ… Job Completed Successfully!\")\n",
        "            st.metric(label=\"Execution Time\", value=f\"{duration:.4f} seconds\")\n",
        "\n",
        "            # ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
        "            if \"Linear Regression\" in algo_type:\n",
        "                st.write(f\"Model Coefficients: {trained_model.coefficients}\")\n",
        "            elif \"K-Means\" in algo_type:\n",
        "                st.write(\"Cluster Centers computed.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a CSV file to begin.\")\n",
        "'''\n",
        "\n",
        "# Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_content)\n",
        "\n",
        "# 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±\n",
        "print(\"Ø¬Ø§Ø±Ù ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚... Ø§Ù†ØªØ¸Ø± Ø§Ù„Ø±Ø§Ø¨Ø· ğŸ‘‡\")\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 &')\n",
        "url = ngrok.connect(8501).public_url\n",
        "print(f\"ğŸ”— Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯: {url}\")"
      ],
      "metadata": {
        "id": "cg1xxwE0tXMC",
        "outputId": "4b4dc536-50ed-4dd5-86d5-020b506774f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø¬Ø§Ø±Ù ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚... Ø§Ù†ØªØ¸Ø± Ø§Ù„Ø±Ø§Ø¨Ø· ğŸ‘‡\n",
            "ğŸ”— Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯: https://pseudooriental-unbaffling-hue.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "!pip install -q pyspark streamlit pyngrok\n",
        "\n",
        "import os\n",
        "import time  # <--- ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ù„Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”´ Ù‡Ø§Ù…: Ø¶Ø¹ Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§\n",
        "NGROK_TOKEN = \"370cvQc2OEKtGFkRfNP3JIaHq3h_37qQ33Xwm3aPugtnLFuSA\"\n",
        "# ==========================================\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Ù…Ø¹ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©: Alaa Yousef & Misk Ashour)\n",
        "project_code = '''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Ø§Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©\n",
        "st.set_page_config(page_title=\"Big Data Project\", layout=\"wide\")\n",
        "\n",
        "# Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©\n",
        "st.sidebar.header(\"Server Configuration\")\n",
        "worker_nodes = st.sidebar.selectbox(\"Number of Worker Nodes (Simulation):\", [1, 2, 4, 8])\n",
        "\n",
        "# ØªÙ‡ÙŠØ¦Ø© Ø¬Ù„Ø³Ø© Ø³Ø¨Ø§Ø±Ùƒ\n",
        "if \"spark_session\" not in st.session_state:\n",
        "    st.session_state.spark_session = SparkSession.builder.master(f\"local[{worker_nodes}]\").appName(\"StudentProject\").getOrCreate()\n",
        "\n",
        "spark = st.session_state.spark_session\n",
        "\n",
        "# --- Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡ ---\n",
        "st.title(\"Cloud-Based Distributed Data Service\")\n",
        "st.markdown(\"**Developed by:** Alaa Yousef & Misk Ashour\")\n",
        "\n",
        "# Ù‚Ø³Ù… Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª\n",
        "st.header(\"1. Upload Dataset\")\n",
        "user_file = st.file_uploader(\"Upload CSV File\", type=\"csv\")\n",
        "\n",
        "if user_file is not None:\n",
        "    pandas_data = pd.read_csv(user_file)\n",
        "    pandas_data = pandas_data.dropna()\n",
        "\n",
        "    spark_df = spark.createDataFrame(pandas_data)\n",
        "    st.session_state[\"main_data\"] = spark_df\n",
        "\n",
        "    st.success(\"File Uploaded Successfully\")\n",
        "    st.write(\"Data Preview:\")\n",
        "    st.dataframe(pandas_data.head())\n",
        "\n",
        "    # Ù‚Ø³Ù… Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª\n",
        "    st.write(\"---\")\n",
        "    st.header(\"2. Descriptive Statistics\")\n",
        "    if st.button(\"Calculate Stats\"):\n",
        "        my_stats = spark_df.describe().toPandas()\n",
        "        st.dataframe(my_stats)\n",
        "\n",
        "    # Ù‚Ø³Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n",
        "    st.write(\"---\")\n",
        "    st.header(\"3. Machine Learning Tasks\")\n",
        "\n",
        "    valid_cols = [c for c, t in spark_df.dtypes if t in ('int', 'double', 'float', 'bigint')]\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        target_variable = st.selectbox(\"Select Target Column (Label):\", valid_cols)\n",
        "    with col2:\n",
        "        input_features = st.multiselect(\"Select Features:\", [c for c in valid_cols if c != target_variable])\n",
        "\n",
        "    selected_model = st.selectbox(\"Choose Algorithm:\",\n",
        "                             [\"Linear Regression\",\n",
        "                              \"Decision Tree Regressor\",\n",
        "                              \"Random Forest Regressor\",\n",
        "                              \"K-Means Clustering\"])\n",
        "\n",
        "    if st.button(\"Start Processing\"):\n",
        "        if not input_features:\n",
        "            st.error(\"Please select at least one feature column\")\n",
        "        else:\n",
        "            assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
        "            prepared_data = assembler.transform(spark_df)\n",
        "\n",
        "            if \"Linear Regression\" in selected_model:\n",
        "                ml_model = LinearRegression(featuresCol=\"features\", labelCol=target_variable)\n",
        "            elif \"Decision Tree\" in selected_model:\n",
        "                ml_model = DecisionTreeRegressor(featuresCol=\"features\", labelCol=target_variable)\n",
        "            elif \"Random Forest\" in selected_model:\n",
        "                ml_model = RandomForestRegressor(featuresCol=\"features\", labelCol=target_variable)\n",
        "            elif \"K-Means\" in selected_model:\n",
        "                ml_model = KMeans(featuresCol=\"features\", k=3)\n",
        "\n",
        "            st.info(f\"Running {selected_model} on {worker_nodes} simulated nodes...\")\n",
        "\n",
        "            t_start = time.time()\n",
        "            final_model = ml_model.fit(prepared_data)\n",
        "            t_end = time.time()\n",
        "            total_time = t_end - t_start\n",
        "\n",
        "            st.success(\"Job Finished\")\n",
        "            st.metric(label=\"Execution Time (Seconds)\", value=f\"{total_time:.4f}\")\n",
        "\n",
        "            if \"Linear Regression\" in selected_model:\n",
        "                st.write(f\"Coefficients: {final_model.coefficients}\")\n",
        "\n",
        "else:\n",
        "    st.warning(\"Please upload a CSV file to start\")\n",
        "'''\n",
        "\n",
        "# Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙˆØªØ´ØºÙŠÙ„Ù‡\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(project_code)\n",
        "\n",
        "print(\"Starting application...\")\n",
        "# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©\n",
        "get_ipython().system_raw('pkill -9 streamlit')\n",
        "time.sleep(2)\n",
        "# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 &')\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"Project Link: {public_url}\")"
      ],
      "metadata": {
        "id": "Qyr0o_Fp-_ux",
        "outputId": "1970115d-e8de-4ad8-821d-a6c829185d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Starting application...\n",
            "Project Link: https://pseudooriental-unbaffling-hue.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "!pip install -q pyspark streamlit pyngrok\n",
        "\n",
        "import os\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”´ Ø¶Ø¹ Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§\n",
        "NGROK_TOKEN = \"370cvQc2OEKtGFkRfNP3JIaHq3h_37qQ33Xwm3aPugtnLFuSA\"\n",
        "# ==========================================\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø·ÙˆØ±\n",
        "project_code = '''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Ø§Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©\n",
        "st.set_page_config(page_title=\"Big Data Project\", layout=\"wide\")\n",
        "\n",
        "# --- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (Styling) ---\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .name-badge {\n",
        "        background-color: #f0f2f6;\n",
        "        border-left: 5px solid #ff4b4b;\n",
        "        padding: 15px;\n",
        "        border-radius: 5px;\n",
        "        text-align: center;\n",
        "        font-family: sans-serif;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .name-text {\n",
        "        color: #31333F;\n",
        "        font-size: 20px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    </style>\n",
        "    <div class=\"name-badge\">\n",
        "        <span class=\"name-text\">âœ¨ Developed by: Alaa Yousef & Misk Ashour âœ¨</span>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©\n",
        "st.sidebar.header(\"Server Configuration\")\n",
        "worker_nodes = st.sidebar.selectbox(\"Number of Worker Nodes (Simulation):\", [1, 2, 4, 8])\n",
        "\n",
        "# ØªÙ‡ÙŠØ¦Ø© Ø³Ø¨Ø§Ø±Ùƒ\n",
        "if \"spark_session\" not in st.session_state:\n",
        "    st.session_state.spark_session = SparkSession.builder.master(f\"local[{worker_nodes}]\").appName(\"StudentProject\").getOrCreate()\n",
        "\n",
        "spark = st.session_state.spark_session\n",
        "\n",
        "st.title(\"â˜ï¸ Cloud-Based Distributed Data Service\")\n",
        "\n",
        "# --- 1. Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª (CSV, JSON, TXT) ---\n",
        "st.header(\"1. Upload Dataset\")\n",
        "# ØªÙ… Ø¥Ø²Ø§Ù„Ø© PDF Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©\n",
        "user_file = st.file_uploader(\"Upload File (CSV, JSON, TXT)\", type=[\"csv\", \"json\", \"txt\"])\n",
        "\n",
        "pandas_data = None\n",
        "\n",
        "if user_file is not None:\n",
        "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ÙˆÙ‚Ø±Ø§Ø¡ØªÙ‡\n",
        "    file_type = user_file.name.split('.')[-1].lower()\n",
        "\n",
        "    try:\n",
        "        if file_type == 'csv':\n",
        "            pandas_data = pd.read_csv(user_file)\n",
        "        elif file_type == 'json':\n",
        "            pandas_data = pd.read_json(user_file)\n",
        "        elif file_type == 'txt':\n",
        "            pandas_data = pd.read_csv(user_file, sep=\"\\t\")\n",
        "\n",
        "        # Ø§Ø°Ø§ Ù†Ø¬Ø­Øª Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "        if pandas_data is not None:\n",
        "            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "            pandas_data = pandas_data.dropna()\n",
        "\n",
        "            # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ Spark\n",
        "            spark_df = spark.createDataFrame(pandas_data)\n",
        "            st.session_state[\"main_data\"] = spark_df\n",
        "\n",
        "            st.success(f\"File ({file_type}) Uploaded Successfully!\")\n",
        "            st.write(\"Data Preview:\")\n",
        "            st.dataframe(pandas_data.head())\n",
        "\n",
        "            # --- 2. Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ---\n",
        "            st.write(\"---\")\n",
        "            st.header(\"2. Descriptive Statistics\")\n",
        "            if st.button(\"Calculate Stats\"):\n",
        "                my_stats = spark_df.describe().toPandas()\n",
        "                st.dataframe(my_stats)\n",
        "\n",
        "            # --- 3. Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ---\n",
        "            st.write(\"---\")\n",
        "            st.header(\"3. Machine Learning Tasks\")\n",
        "\n",
        "            valid_cols = [c for c, t in spark_df.dtypes if t in ('int', 'double', 'float', 'bigint')]\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                target_variable = st.selectbox(\"Select Target Column (Label):\", valid_cols)\n",
        "            with col2:\n",
        "                input_features = st.multiselect(\"Select Features:\", [c for c in valid_cols if c != target_variable])\n",
        "\n",
        "            selected_model = st.selectbox(\"Choose Algorithm:\",\n",
        "                                     [\"Linear Regression\",\n",
        "                                      \"Decision Tree Regressor\",\n",
        "                                      \"Random Forest Regressor\",\n",
        "                                      \"K-Means Clustering\"])\n",
        "\n",
        "            if st.button(\"Start Processing\"):\n",
        "                if not input_features:\n",
        "                    st.error(\"Please select at least one feature column\")\n",
        "                else:\n",
        "                    assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
        "                    prepared_data = assembler.transform(spark_df)\n",
        "\n",
        "                    if \"Linear Regression\" in selected_model:\n",
        "                        ml_model = LinearRegression(featuresCol=\"features\", labelCol=target_variable)\n",
        "                    elif \"Decision Tree\" in selected_model:\n",
        "                        ml_model = DecisionTreeRegressor(featuresCol=\"features\", labelCol=target_variable)\n",
        "                    elif \"Random Forest\" in selected_model:\n",
        "                        ml_model = RandomForestRegressor(featuresCol=\"features\", labelCol=target_variable)\n",
        "                    elif \"K-Means\" in selected_model:\n",
        "                        ml_model = KMeans(featuresCol=\"features\", k=3)\n",
        "\n",
        "                    st.info(f\"Running {selected_model} on {worker_nodes} simulated nodes...\")\n",
        "\n",
        "                    t_start = time.time()\n",
        "                    final_model = ml_model.fit(prepared_data)\n",
        "                    t_end = time.time()\n",
        "                    total_time = t_end - t_start\n",
        "\n",
        "                    st.success(\"Job Finished\")\n",
        "                    st.metric(label=\"Execution Time (Seconds)\", value=f\"{total_time:.4f}\")\n",
        "\n",
        "                    if \"Linear Regression\" in selected_model:\n",
        "                        st.write(f\"Coefficients: {final_model.coefficients}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a dataset (CSV, JSON, or TXT) to start.\")\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(project_code)\n",
        "\n",
        "print(\"Starting application...\")\n",
        "get_ipython().system_raw('pkill -9 streamlit')\n",
        "time.sleep(2)\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 &')\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"Project Link: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIVjt3etdD5O",
        "outputId": "4ff5ef9c-5f76-4340-e749-13d488e10c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Starting application...\n",
            "Project Link: https://pseudooriental-unbaffling-hue.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "!pip install -q pyspark streamlit pyngrok\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json # <--- Ù…ÙƒØªØ¨Ø© Ø¬Ø¯ÙŠØ¯Ø©\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”´ Ø¶Ø¹ Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§\n",
        "NGROK_TOKEN = \"370cvQc2OEKtGFkRfNP3JIaHq3h_37qQ33Xwm3aPugtnLFuSA\"\n",
        "# ==========================================\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø·ÙˆØ±\n",
        "project_code = '''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Ø§Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©\n",
        "st.set_page_config(page_title=\"Big Data Project\", layout=\"wide\")\n",
        "\n",
        "# --- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ---\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .name-badge {\n",
        "        background-color: #f0f2f6;\n",
        "        border-left: 5px solid #ff4b4b;\n",
        "        padding: 15px;\n",
        "        border-radius: 5px;\n",
        "        text-align: center;\n",
        "        font-family: sans-serif;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .name-text {\n",
        "        color: #31333F;\n",
        "        font-size: 20px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    </style>\n",
        "    <div class=\"name-badge\">\n",
        "        <span class=\"name-text\">âœ¨ Developed by: Alaa Yousef & Misk Ashour âœ¨</span>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©\n",
        "st.sidebar.header(\"Server Configuration\")\n",
        "worker_nodes = st.sidebar.selectbox(\"Number of Worker Nodes (Simulation):\", [1, 2, 4, 8])\n",
        "\n",
        "# ØªÙ‡ÙŠØ¦Ø© Ø³Ø¨Ø§Ø±Ùƒ\n",
        "if \"spark_session\" not in st.session_state:\n",
        "    st.session_state.spark_session = SparkSession.builder.master(f\"local[{worker_nodes}]\").appName(\"StudentProject\").getOrCreate()\n",
        "\n",
        "spark = st.session_state.spark_session\n",
        "\n",
        "st.title(\"â˜ï¸ Cloud-Based Distributed Data Service\")\n",
        "\n",
        "# --- 1. Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ---\n",
        "st.header(\"1. Upload Dataset\")\n",
        "user_file = st.file_uploader(\"Upload File (CSV, JSON, TXT)\", type=[\"csv\", \"json\", \"txt\"])\n",
        "\n",
        "pandas_data = None\n",
        "\n",
        "if user_file is not None:\n",
        "    file_type = user_file.name.split('.')[-1].lower()\n",
        "\n",
        "    try:\n",
        "        if file_type == 'csv':\n",
        "            pandas_data = pd.read_csv(user_file)\n",
        "        elif file_type == 'json':\n",
        "            # --- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù‚Ø±Ø§Ø¡Ø© JSON ---\n",
        "            # Ù†Ù‚Ø±Ø£ Ø§Ù„Ù…Ù„Ù ÙƒÙ…Ù„Ù Ù†ØµÙŠ Ø¹Ø§Ø¯ÙŠ Ø£ÙˆÙ„Ø§Ù‹\n",
        "            json_content = json.load(user_file)\n",
        "            # Ø¥Ø°Ø§ ÙƒØ§Ù† ØµÙØ§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ (Dictionary)ØŒ Ù†Ø­ÙˆÙ„Ù‡ Ù„Ù‚Ø§Ø¦Ù…Ø©\n",
        "            if isinstance(json_content, dict):\n",
        "                json_content = [json_content]\n",
        "            pandas_data = pd.DataFrame(json_content)\n",
        "            # -------------------------------\n",
        "        elif file_type == 'txt':\n",
        "            pandas_data = pd.read_csv(user_file, sep=\"\\t\")\n",
        "\n",
        "        if pandas_data is not None:\n",
        "            pandas_data = pandas_data.dropna()\n",
        "            spark_df = spark.createDataFrame(pandas_data)\n",
        "            st.session_state[\"main_data\"] = spark_df\n",
        "\n",
        "            st.success(f\"File ({file_type}) Uploaded Successfully!\")\n",
        "            st.write(\"Data Preview:\")\n",
        "            st.dataframe(pandas_data.head())\n",
        "\n",
        "            # --- 2. Ø§Ù„Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª ---\n",
        "            st.write(\"---\")\n",
        "            st.header(\"2. Descriptive Statistics\")\n",
        "            if st.button(\"Calculate Stats\"):\n",
        "                my_stats = spark_df.describe().toPandas()\n",
        "                st.dataframe(my_stats)\n",
        "\n",
        "            # --- 3. Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ---\n",
        "            st.write(\"---\")\n",
        "            st.header(\"3. Machine Learning Tasks\")\n",
        "\n",
        "            valid_cols = [c for c, t in spark_df.dtypes if t in ('int', 'double', 'float', 'bigint')]\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                target_variable = st.selectbox(\"Select Target Column (Label):\", valid_cols)\n",
        "            with col2:\n",
        "                input_features = st.multiselect(\"Select Features:\", [c for c in valid_cols if c != target_variable])\n",
        "\n",
        "            selected_model = st.selectbox(\"Choose Algorithm:\",\n",
        "                                     [\"Linear Regression\",\n",
        "                                      \"Decision Tree Regressor\",\n",
        "                                      \"Random Forest Regressor\",\n",
        "                                      \"K-Means Clustering\"])\n",
        "\n",
        "            if st.button(\"Start Processing\"):\n",
        "                if not input_features:\n",
        "                    st.error(\"Please select at least one feature column\")\n",
        "                else:\n",
        "                    assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
        "                    prepared_data = assembler.transform(spark_df)\n",
        "\n",
        "                    if \"Linear Regression\" in selected_model:\n",
        "                        ml_model = LinearRegression(featuresCol=\"features\", labelCol=target_variable)\n",
        "                    elif \"Decision Tree\" in selected_model:\n",
        "                        ml_model = DecisionTreeRegressor(featuresCol=\"features\", labelCol=target_variable)\n",
        "                    elif \"Random Forest\" in selected_model:\n",
        "                        ml_model = RandomForestRegressor(featuresCol=\"features\", labelCol=target_variable)\n",
        "                    elif \"K-Means\" in selected_model:\n",
        "                        ml_model = KMeans(featuresCol=\"features\", k=3)\n",
        "\n",
        "                    st.info(f\"Running {selected_model} on {worker_nodes} simulated nodes...\")\n",
        "\n",
        "                    t_start = time.time()\n",
        "                    final_model = ml_model.fit(prepared_data)\n",
        "                    t_end = time.time()\n",
        "                    total_time = t_end - t_start\n",
        "\n",
        "                    st.success(\"Job Finished\")\n",
        "                    st.metric(label=\"Execution Time (Seconds)\", value=f\"{total_time:.4f}\")\n",
        "\n",
        "                    if \"Linear Regression\" in selected_model:\n",
        "                        st.write(f\"Coefficients: {final_model.coefficients}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a dataset to start.\")\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(project_code)\n",
        "\n",
        "print(\"Starting application...\")\n",
        "get_ipython().system_raw('pkill -9 streamlit')\n",
        "time.sleep(2)\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 &')\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"Project Link: {public_url}\")"
      ],
      "metadata": {
        "id": "j05O4RdBoJ1y",
        "outputId": "80467209-9ba2-416a-a2cc-debddaefca39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting application...\n",
            "Project Link: https://pseudooriental-unbaffling-hue.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©\n",
        "!pip install -q pyspark streamlit pyngrok\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”´ Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ (ØªÙ… Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„ÙŠÙ‡ ÙƒÙ…Ø§ Ù‡Ùˆ)\n",
        "NGROK_TOKEN = \"370cvQc2OEKtGFkRfNP3JIaHq3h_37qQ33Xwm3aPugtnLFuSA\"\n",
        "# ==========================================\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# 2. ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø·ÙˆØ± ÙˆØ§Ù„Ø´Ø§Ù…Ù„\n",
        "project_code = '''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©\n",
        "st.set_page_config(page_title=\"Big Data Project\", layout=\"wide\")\n",
        "\n",
        "# --- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (CSS) ---\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .name-badge {\n",
        "        background-color: #f0f2f6;\n",
        "        border-left: 5px solid #ff4b4b;\n",
        "        padding: 15px;\n",
        "        border-radius: 5px;\n",
        "        text-align: center;\n",
        "        font-family: sans-serif;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .name-text {\n",
        "        color: #31333F;\n",
        "        font-size: 20px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .metric-card {\n",
        "        background-color: #ffffff;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
        "        text-align: center;\n",
        "    }\n",
        "    </style>\n",
        "    <div class=\"name-badge\">\n",
        "        <span class=\"name-text\">âœ¨ Developed by: Alaa Yousef & Misk Ashour âœ¨</span>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ØªÙ‡ÙŠØ¦Ø© Spark Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©\n",
        "# ---------------------------------------------------------\n",
        "if \"spark\" not in st.session_state:\n",
        "    st.session_state.spark = SparkSession.builder.master(\"local[1]\").appName(\"StudentProject\").getOrCreate()\n",
        "spark = st.session_state.spark\n",
        "\n",
        "st.title(\"â˜ï¸ Cloud-Based Distributed Data Service\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Upload Dataset)\n",
        "# ---------------------------------------------------------\n",
        "st.header(\"1. Upload Dataset (Big Data Support)\")\n",
        "st.markdown(\"Supports CSV, JSON, and TXT formats. Recommended: UCI Machine Learning Repository Datasets.\")\n",
        "\n",
        "user_file = st.file_uploader(\"Upload File\", type=[\"csv\", \"json\", \"txt\"])\n",
        "\n",
        "# Ù…ØªØºÙŠØ± Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (Pandas) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„\n",
        "if \"pandas_data\" not in st.session_state:\n",
        "    st.session_state.pandas_data = None\n",
        "\n",
        "if user_file is not None:\n",
        "    file_type = user_file.name.split('.')[-1].lower()\n",
        "    try:\n",
        "        if file_type == 'csv':\n",
        "            data = pd.read_csv(user_file)\n",
        "        elif file_type == 'json':\n",
        "            json_content = json.load(user_file)\n",
        "            if isinstance(json_content, dict): json_content = [json_content]\n",
        "            data = pd.DataFrame(json_content)\n",
        "        elif file_type == 'txt':\n",
        "            data = pd.read_csv(user_file, sep=\"\\t\")\n",
        "\n",
        "        # Ø­ÙØ¸ Ù†Ø³Ø®Ø© Pandas ÙÙŠ Session State\n",
        "        st.session_state.pandas_data = data.dropna()\n",
        "\n",
        "        # Ø¥Ù†Ø´Ø§Ø¡ Spark DataFrame\n",
        "        spark_df = spark.createDataFrame(st.session_state.pandas_data)\n",
        "        st.session_state[\"spark_df\"] = spark_df\n",
        "\n",
        "        st.success(f\"File ({user_file.name}) Uploaded Successfully!\")\n",
        "\n",
        "        with st.expander(\"ğŸ“„ Click to View Raw Data Preview\"):\n",
        "            st.dataframe(data.head(10))\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©\n",
        "if st.session_state.pandas_data is not None:\n",
        "    spark_df = st.session_state[\"spark_df\"]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙÙŠØ© (Descriptive Statistics)\n",
        "    # ---------------------------------------------------------\n",
        "    st.write(\"---\")\n",
        "    st.header(\"2. Descriptive Statistics & Data Profiling\")\n",
        "\n",
        "    if st.button(\"Generate Statistics Report\"):\n",
        "        # Ø£) Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ ÙˆØ§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        num_rows = spark_df.count()\n",
        "        num_cols = len(spark_df.columns)\n",
        "\n",
        "        col1.metric(\"Total Rows\", num_rows)\n",
        "        col2.metric(\"Total Columns\", num_cols)\n",
        "\n",
        "        # Ø¨) Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "        st.subheader(\"Data Types Schema\")\n",
        "        st.json(dict(spark_df.dtypes))\n",
        "\n",
        "        # Ø¬) Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (Missing Values)\n",
        "        st.subheader(\"Missing Values Analysis\")\n",
        "        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯\n",
        "        missing_counts = {}\n",
        "        for c in spark_df.columns:\n",
        "            # Ù†Ø¹Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªÙŠ ØªØ³Ø§ÙˆÙŠ null Ø£Ùˆ NaN\n",
        "            count = spark_df.filter(spark_df[c].isNull()).count()\n",
        "            if count > 0:\n",
        "                missing_counts[c] = count\n",
        "\n",
        "        if missing_counts:\n",
        "            st.write(missing_counts)\n",
        "        else:\n",
        "            st.info(\"No missing values detected in the dataset.\")\n",
        "\n",
        "        # Ø¯) Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©\n",
        "        st.subheader(\"Summary Statistics\")\n",
        "        st.dataframe(spark_df.describe().toPandas())\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ (4 Algorithms)\n",
        "    # ---------------------------------------------------------\n",
        "    st.write(\"---\")\n",
        "    st.header(\"3. Distributed Machine Learning Jobs\")\n",
        "    st.markdown(\"Runs 4 algorithms simultaneously: **Linear Regression, Decision Tree, Random Forest, and K-Means**.\")\n",
        "\n",
        "    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "    valid_cols = [c for c, t in spark_df.dtypes if t in ('int', 'double', 'float', 'bigint', 'long')]\n",
        "\n",
        "    c1, c2 = st.columns(2)\n",
        "    with c1:\n",
        "        target_col = st.selectbox(\"Select Target Label (Y):\", valid_cols)\n",
        "    with c2:\n",
        "        feature_cols = st.multiselect(\"Select Features (X):\", [c for c in valid_cols if c != target_col])\n",
        "\n",
        "    if st.button(\"Run All ML Models\"):\n",
        "        if not feature_cols:\n",
        "            st.error(\"Please select at least one feature column!\")\n",
        "        else:\n",
        "            # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Vector Assembly)\n",
        "            assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "            prepared_data = assembler.transform(spark_df).select(\"features\", target_col)\n",
        "            # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ³Ù…ÙŠØ© Ù„ØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ø¬Ù…ÙŠØ¹\n",
        "            prepared_data = prepared_data.withColumnRenamed(target_col, \"label\")\n",
        "\n",
        "            # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©\n",
        "            models = {\n",
        "                \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                \"Random Forest\": RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                \"K-Means (Clustering)\": KMeans(featuresCol=\"features\", k=3)\n",
        "            }\n",
        "\n",
        "            st.write(\"### ğŸš€ Execution Results:\")\n",
        "            results_list = []\n",
        "\n",
        "            # Ø­Ù„Ù‚Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "            for name, model in models.items():\n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    trained_model = model.fit(prepared_data)\n",
        "                    end_time = time.time()\n",
        "                    duration = end_time - start_time\n",
        "                    st.success(f\"âœ… **{name}** completed in **{duration:.4f} seconds**\")\n",
        "                    results_list.append({\"Algorithm\": name, \"Time (s)\": duration})\n",
        "                except Exception as e:\n",
        "                    st.error(f\"âŒ {name} Failed: {e}\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹ ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡ (Scalability Test)\n",
        "    # ---------------------------------------------------------\n",
        "    st.write(\"---\")\n",
        "    st.header(\"4. Scalability & Performance Analysis\")\n",
        "    st.markdown(\"This test simulates a cluster environment by running a job on **1, 2, 4, and 8 nodes** iteratively.\")\n",
        "\n",
        "    if st.button(\"âš¡ Run Scalability Test (1, 2, 4, 8 Nodes)\"):\n",
        "        if not feature_cols:\n",
        "             st.error(\"Please select features in Section 3 first.\")\n",
        "        else:\n",
        "            performance_data = []\n",
        "            node_counts = [1, 2, 4, 8]\n",
        "\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            # Ù†Ø­ØªØ§Ø¬ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ø¬Ù„Ø³Ø© Spark Ø¬Ø¯ÙŠØ¯Ø©\n",
        "            raw_pandas = st.session_state.pandas_data\n",
        "\n",
        "            for idx, n in enumerate(node_counts):\n",
        "                status_text.text(f\"Simulation: Provisioning Cluster with {n} Nodes...\")\n",
        "\n",
        "                # 1. Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙˆØ¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¹Ø¯Ø¯ Nodes Ù…Ø®ØªÙ„Ù\n",
        "                st.session_state.spark.stop()\n",
        "                new_spark = SparkSession.builder.master(f\"local[{n}]\").appName(f\"Sim_Node_{n}\").getOrCreate()\n",
        "                st.session_state.spark = new_spark # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©\n",
        "\n",
        "                # 2. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ø¯ÙŠØ¯ (Ù„Ø£Ù† Ø§Ù„Ø¬Ù„Ø³Ø© ØªØºÙŠØ±Øª)\n",
        "                temp_df = new_spark.createDataFrame(raw_pandas)\n",
        "                vec_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "                ready_data = vec_assembler.transform(temp_df).select(\"features\", target_col).withColumnRenamed(target_col, \"label\")\n",
        "\n",
        "                # 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø© (Ù†Ø³ØªØ®Ø¯Ù… Linear Regression ÙƒÙ…Ø¹ÙŠØ§Ø±)\n",
        "                lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "                start_t = time.time()\n",
        "                model = lr.fit(ready_data)\n",
        "                end_t = time.time()\n",
        "\n",
        "                exec_time = end_t - start_t\n",
        "                performance_data.append((n, exec_time))\n",
        "\n",
        "                # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…\n",
        "                progress_bar.progress((idx + 1) / len(node_counts))\n",
        "\n",
        "            status_text.text(\"Simulation Complete!\")\n",
        "\n",
        "            # --- Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ (Speedup & Efficiency) ---\n",
        "            # Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Base Time) Ù‡Ùˆ Ø²Ù…Ù† 1 Node\n",
        "            t_base = performance_data[0][1]\n",
        "\n",
        "            final_results = []\n",
        "            for n, t in performance_data:\n",
        "                speedup = t_base / t if t > 0 else 0\n",
        "                efficiency = speedup / n\n",
        "                final_results.append({\n",
        "                    \"Nodes\": n,\n",
        "                    \"Execution Time (s)\": round(t, 4),\n",
        "                    \"Speedup\": round(speedup, 2),\n",
        "                    \"Efficiency\": round(efficiency, 2)\n",
        "                })\n",
        "\n",
        "            # Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯ÙˆÙ„\n",
        "            df_perf = pd.DataFrame(final_results)\n",
        "            st.subheader(\"ğŸ“Š Performance Table\")\n",
        "            st.dataframe(df_perf)\n",
        "\n",
        "            # --- 5. Ø§Ù„Ø­ÙØ¸ ÙÙŠ Cloud Storage (Ù…Ø­Ø§ÙƒØ§Ø©) ---\n",
        "            os.makedirs(\"cloud_results\", exist_ok=True)\n",
        "            csv_path = \"cloud_results/performance_metrics.csv\"\n",
        "            df_perf.to_csv(csv_path, index=False)\n",
        "\n",
        "            st.success(f\"âœ… Results saved to Cloud Storage: `{csv_path}`\")\n",
        "            st.info(\"ğŸ’¡ Note: In a real cluster, 'Speedup' increases with nodes. In this local simulation, overhead might cause slight delays.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a dataset to begin.\")\n",
        "'''\n",
        "\n",
        "# ÙƒØªØ§Ø¨Ø© Ù…Ù„Ù Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(project_code)\n",
        "\n",
        "# 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±\n",
        "print(\"... Ø¬Ø§Ø±Ù Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ...\")\n",
        "get_ipython().system_raw('pkill -9 streamlit')\n",
        "time.sleep(2)\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 &')\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"ğŸ”— Ø±Ø§Ø¨Ø· Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø§Ù„Ø¬Ø¯ÙŠØ¯: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx8Zv-s7_euz",
        "outputId": "a7eff992-9d5c-4828-88d8-d5ea008c3a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "... Ø¬Ø§Ø±Ù Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ...\n",
            "ğŸ”— Ø±Ø§Ø¨Ø· Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø§Ù„Ø¬Ø¯ÙŠØ¯: https://pseudooriental-unbaffling-hue.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©\n",
        "!pip install -q pyspark streamlit pyngrok\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”´ Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ\n",
        "NGROK_TOKEN = \"370cvQc2OEKtGFkRfNP3JIaHq3h_37qQ33Xwm3aPugtnLFuSA\"\n",
        "# ==========================================\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# 2. ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø·ÙˆØ± ÙˆØ§Ù„Ø´Ø§Ù…Ù„ (Ù…Ø¹ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©)\n",
        "project_code = '''\n",
        "import streamlit as st\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©\n",
        "st.set_page_config(page_title=\"Big Data Project\", layout=\"wide\")\n",
        "\n",
        "# --- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ (CSS) ---\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .name-badge {\n",
        "        background-color: #f0f2f6;\n",
        "        border-left: 5px solid #ff4b4b;\n",
        "        padding: 15px;\n",
        "        border-radius: 5px;\n",
        "        text-align: center;\n",
        "        font-family: sans-serif;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .name-text {\n",
        "        color: #31333F;\n",
        "        font-size: 20px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    </style>\n",
        "    <div class=\"name-badge\">\n",
        "        <span class=\"name-text\">âœ¨ Developed by: Alaa Yousef & Misk Ashour âœ¨</span>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ØªÙ‡ÙŠØ¦Ø© Spark Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©\n",
        "# ---------------------------------------------------------\n",
        "if \"spark\" not in st.session_state:\n",
        "    st.session_state.spark = SparkSession.builder.master(\"local[1]\").appName(\"StudentProject\").getOrCreate()\n",
        "spark = st.session_state.spark\n",
        "\n",
        "st.title(\"â˜ï¸ Cloud-Based Distributed Data Service\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Upload Dataset)\n",
        "# ---------------------------------------------------------\n",
        "st.header(\"1. Upload Dataset (Big Data Support)\")\n",
        "st.markdown(\"Supports CSV, JSON, and TXT formats.\")\n",
        "\n",
        "user_file = st.file_uploader(\"Upload File\", type=[\"csv\", \"json\", \"txt\"])\n",
        "\n",
        "# Ù…ØªØºÙŠØ± Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…\n",
        "if \"pandas_data\" not in st.session_state:\n",
        "    st.session_state.pandas_data = None\n",
        "\n",
        "if user_file is not None:\n",
        "    file_type = user_file.name.split('.')[-1].lower()\n",
        "    try:\n",
        "        if file_type == 'csv':\n",
        "            data = pd.read_csv(user_file)\n",
        "        elif file_type == 'json':\n",
        "            json_content = json.load(user_file)\n",
        "            if isinstance(json_content, dict): json_content = [json_content]\n",
        "            data = pd.DataFrame(json_content)\n",
        "        elif file_type == 'txt':\n",
        "            data = pd.read_csv(user_file, sep=\"\\t\")\n",
        "\n",
        "        # Ø­ÙØ¸ Ù†Ø³Ø®Ø© Pandas\n",
        "        st.session_state.pandas_data = data.dropna()\n",
        "\n",
        "        # Ø¥Ù†Ø´Ø§Ø¡ Spark DataFrame\n",
        "        spark_df = spark.createDataFrame(st.session_state.pandas_data)\n",
        "        st.session_state[\"spark_df\"] = spark_df\n",
        "\n",
        "        st.success(f\"File ({user_file.name}) Uploaded Successfully!\")\n",
        "\n",
        "        with st.expander(\"ğŸ“„ Click to View Raw Data Preview\"):\n",
        "            st.dataframe(data.head(10))\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©\n",
        "if st.session_state.pandas_data is not None:\n",
        "    spark_df = st.session_state[\"spark_df\"]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙÙŠØ©\n",
        "    # ---------------------------------------------------------\n",
        "    st.write(\"---\")\n",
        "    st.header(\"2. Descriptive Statistics & Data Profiling\")\n",
        "\n",
        "    if st.button(\"Generate Statistics Report\"):\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        num_rows = spark_df.count()\n",
        "        num_cols = len(spark_df.columns)\n",
        "\n",
        "        col1.metric(\"Total Rows\", num_rows)\n",
        "        col2.metric(\"Total Columns\", num_cols)\n",
        "\n",
        "        st.subheader(\"Data Types Schema\")\n",
        "        st.json(dict(spark_df.dtypes))\n",
        "\n",
        "        st.subheader(\"Summary Statistics\")\n",
        "        st.dataframe(spark_df.describe().toPandas())\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ\n",
        "    # ---------------------------------------------------------\n",
        "    st.write(\"---\")\n",
        "    st.header(\"3. Distributed Machine Learning Jobs\")\n",
        "    st.markdown(\"Runs 4 algorithms simultaneously: **Linear Regression, Decision Tree, Random Forest, and K-Means**.\")\n",
        "\n",
        "    valid_cols = [c for c, t in spark_df.dtypes if t in ('int', 'double', 'float', 'bigint', 'long')]\n",
        "\n",
        "    c1, c2 = st.columns(2)\n",
        "    with c1:\n",
        "        target_col = st.selectbox(\"Select Target Label (Y):\", valid_cols)\n",
        "    with c2:\n",
        "        feature_cols = st.multiselect(\"Select Features (X):\", [c for c in valid_cols if c != target_col])\n",
        "\n",
        "    if st.button(\"Run All ML Models\"):\n",
        "        if not feature_cols:\n",
        "            st.error(\"Please select at least one feature column!\")\n",
        "        else:\n",
        "            assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "            prepared_data = assembler.transform(spark_df).select(\"features\", target_col)\n",
        "            prepared_data = prepared_data.withColumnRenamed(target_col, \"label\")\n",
        "\n",
        "            models = {\n",
        "                \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                \"Random Forest\": RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\"),\n",
        "                \"K-Means (Clustering)\": KMeans(featuresCol=\"features\", k=3)\n",
        "            }\n",
        "\n",
        "            st.write(\"### ğŸš€ Execution Results:\")\n",
        "            for name, model in models.items():\n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    trained_model = model.fit(prepared_data)\n",
        "                    duration = time.time() - start_time\n",
        "                    st.success(f\"âœ… **{name}** completed in **{duration:.4f} seconds**\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"âŒ {name} Failed: {e}\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹ ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡ (Scalability Test)\n",
        "    # ---------------------------------------------------------\n",
        "    st.write(\"---\")\n",
        "    st.header(\"4. Scalability & Performance Analysis\")\n",
        "    st.markdown(\"This test simulates a cluster environment by running a job on **1, 2, 4, and 8 nodes** iteratively.\")\n",
        "\n",
        "    if st.button(\"âš¡ Run Scalability Test (1, 2, 4, 8 Nodes)\"):\n",
        "        if not feature_cols:\n",
        "             st.error(\"Please select features in Section 3 first.\")\n",
        "        else:\n",
        "            performance_data = []\n",
        "            node_counts = [1, 2, 4, 8]\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "            raw_pandas = st.session_state.pandas_data\n",
        "\n",
        "            for idx, n in enumerate(node_counts):\n",
        "                status_text.text(f\"Simulation: Provisioning Cluster with {n} Nodes...\")\n",
        "\n",
        "                # Ø¥ÙŠÙ‚Ø§Ù ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Spark Ø¨Ø¹Ø¯Ø¯ Nodes Ø¬Ø¯ÙŠØ¯\n",
        "                st.session_state.spark.stop()\n",
        "                new_spark = SparkSession.builder.master(f\"local[{n}]\").appName(f\"Sim_Node_{n}\").getOrCreate()\n",
        "                st.session_state.spark = new_spark\n",
        "\n",
        "                # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "                temp_df = new_spark.createDataFrame(raw_pandas)\n",
        "                vec_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "                ready_data = vec_assembler.transform(temp_df).select(\"features\", target_col).withColumnRenamed(target_col, \"label\")\n",
        "\n",
        "                # Ù‚ÙŠØ§Ø³ Ø§Ù„Ø²Ù…Ù†\n",
        "                lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "                start_t = time.time()\n",
        "                model = lr.fit(ready_data)\n",
        "                end_t = time.time()\n",
        "\n",
        "                performance_data.append((n, end_t - start_t))\n",
        "                progress_bar.progress((idx + 1) / len(node_counts))\n",
        "\n",
        "            status_text.text(\"Simulation Complete!\")\n",
        "\n",
        "            # Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡\n",
        "            t_base = performance_data[0][1]\n",
        "            final_results = []\n",
        "            for n, t in performance_data:\n",
        "                speedup = t_base / t if t > 0 else 0\n",
        "                efficiency = speedup / n\n",
        "                final_results.append({\n",
        "                    \"Nodes\": n,\n",
        "                    \"Execution Time (s)\": round(t, 4),\n",
        "                    \"Speedup\": round(speedup, 2),\n",
        "                    \"Efficiency\": round(efficiency, 2)\n",
        "                })\n",
        "\n",
        "            df_perf = pd.DataFrame(final_results)\n",
        "            st.subheader(\"ğŸ“Š Performance Table\")\n",
        "            st.dataframe(df_perf)\n",
        "\n",
        "            # --- ğŸ”´ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù‡Ù†Ø§ ğŸ”´ ---\n",
        "            st.write(\"---\")\n",
        "            st.subheader(\"ğŸ“ˆ Visual Analysis\")\n",
        "\n",
        "            chart1, chart2 = st.columns(2)\n",
        "\n",
        "            with chart1:\n",
        "                st.markdown(\"##### Execution Time (Lower is Better)\")\n",
        "                # Ø±Ø³Ù… Ø®Ø·ÙŠ Ù„Ù„Ø²Ù…Ù†\n",
        "                st.line_chart(df_perf.set_index(\"Nodes\")[\"Execution Time (s)\"])\n",
        "\n",
        "            with chart2:\n",
        "                st.markdown(\"##### Speedup Factor (Higher is Better)\")\n",
        "                # Ø±Ø³Ù… Ø¹Ù…ÙˆØ¯ÙŠ Ù„Ù„ØªØ³Ø±ÙŠØ¹\n",
        "                st.bar_chart(df_perf.set_index(\"Nodes\")[\"Speedup\"])\n",
        "\n",
        "            # Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù CSV\n",
        "            os.makedirs(\"cloud_results\", exist_ok=True)\n",
        "            csv_path = \"cloud_results/performance_metrics.csv\"\n",
        "            df_perf.to_csv(csv_path, index=False)\n",
        "            st.success(f\"âœ… Results saved to Cloud Storage: `{csv_path}`\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a dataset to begin.\")\n",
        "'''\n",
        "\n",
        "# ÙƒØªØ§Ø¨Ø© Ù…Ù„Ù Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(project_code)\n",
        "\n",
        "# 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±\n",
        "print(\"... Ø¬Ø§Ø±Ù Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ...\")\n",
        "get_ipython().system_raw('pkill -9 streamlit')\n",
        "time.sleep(2)\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.port 8501 &')\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"ğŸ”— Ø±Ø§Ø¨Ø· Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø§Ù„Ø¬Ø¯ÙŠØ¯: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhJ1UXTgWGwF",
        "outputId": "84dbee98-4ef1-4304-9fc6-f7b89188c5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "... Ø¬Ø§Ø±Ù Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ...\n",
            "ğŸ”— Ø±Ø§Ø¨Ø· Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø§Ù„Ø¬Ø¯ÙŠØ¯: https://pseudooriental-unbaffling-hue.ngrok-free.dev\n"
          ]
        }
      ]
    }
  ]
}